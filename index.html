
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
    <script src="js/google-code-prettify/prettify.js"></script> -->
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion</h2>
            <h5 style="color:#5a6268;">Submitted to IROS 2024</h5>
            <hr>
            <h6 style="line-height: 26px;">
              <a href="" target="_blank">Guiyong Zheng</a><sup>1,4,*</sup>,
              <a href="" target="_blank">Jinqi Jiang</a><sup>1,3,*</sup>,
              <a href="https://chen-albert-feng.github.io/AlbertFeng.github.io/" target="_blank">Chen Feng</a><sup>2,*</sup>,
              <a href="https://uav.hkust.edu.hk/group/" target="_blank">Shaojie Shen</a><sup>2</sup>,
              <a href="http://sysu-star.com/" target="_blank">Boyu Zhou</a><sup>1,†</sup>
            </h6>
          <p>
            <h7 style="line-height: 2px;">
              <sup>1</sup> Sun Yat-Sen University. &nbsp;&nbsp;
              <sup>2</sup> The Hong Kong University of Science and Technology. &nbsp;&nbsp;
              <br>
              <sup>3</sup> Harbin Institute of Technology. &nbsp;&nbsp;
              <sup>4</sup> Xidian University. &nbsp;&nbsp;
              <br>
            </h7>
              <sup>*</sup>Equal Contribution &nbsp;&nbsp;
              <sup>†</sup>Corresponding Authors
          </p>

          <div class="row justify-content-center">
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/SYSU-STAR/MASSTAR" role="button"  target="_blank">
                  <i class="fa fa-github-alt"></i> Code </a> </p>
            </div>
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/SYSU-STAR/MASSTAR" role="button"  target="_blank">
                  <i class="fa fa-database"></i> Datasets </a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.bilibili.com/video/BV1fy4y1F7Z6/?spm_id_from=333.999.0.0&vd_source=7d9ba13550e9ec24b6bf69d5c3ff3b37" role="button"  target="_blank">
                <i class="fa fa-video-camera"></i> Video </a> </p>
          </div>
            <!-- <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/SYSU-STAR/MASSTAR" role="button"  target="_blank">
                  <i class="fa fa-desktop"></i> Demo </a> </p>
            </div> -->
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h4>Abstract</h4>
                <hr style="margin-top:0px">
                <h6 style="color:#8899a5" class="text-left"> 
                  We propose <strong>MASSTAR</strong>, a <strong>m</strong>ulti-modal l<strong>a</strong>rge-scale <strong>s</strong>cene dataset with a ver<strong>s</strong>atile <strong>t</strong>oolchain for surf<strong>a</strong>ce p<strong>r</strong>ediction and completion. 
                </h6>
                  <img src="assets/imgs/head_new.jpg" alt="Overview" width="70%">
                  <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                      <source src="video/teaser.mp4" type="video/mp4">
                  </video> -->
                  <br>
                  <br>
              <p class="text-left">
                <strong>
                &nbsp;&nbsp;Surface prediction and completion have been widely studied in various applications for diverse downstream tasks. With the development of related research, surface completion algorithms have been evolving from object-level to scene-level, from virtual scenes to the real world, and from single modality to multi-modality. Current datasets exhibit unsatisfactory performance and a lack of challenge since they do not cater to the trends of research, significantly impeding the development of this field. In this paper, 
                We developed a versatile toolchain to facilitate processing the data by segmenting the raw 3D data, selecting the valuable model from the raw 3D data, and generating multi-modal data, including RGB images, descriptive texts, depth images, and partial point clouds. Then we created an example dataset using the toolchain with a large number of collected scene-level models, including portions of real-world captured data from a wide range of open-source projects.
                Additionally, we benchmarked different surface completion algorithms trained on our dataset and gave a comprehensive result. We compared MASSTAR with the current dataset, which validates the superiority of our dataset-toolchain system.
              </strong>
              </p>
            </div>
          </div>
        </div>
    </div>
  </section>
  <!-- <br> -->

<!-- Method -->
  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h4>Multi-modal Dataset Generation Toolchain</h4>
                <hr style="margin-top:0px">
                <!-- <h6 style="color:#8899a5" class="text-left"> 
                  TGS enables fast reconstruction from single-view image. It builds the 3D representation upon a hybrid Triplane-Gaussian representation by evaluating a transformer-based framework, from which 3D Gaussians would be decoded.
                </h6> -->
                
                <h5  class="text-center"> 
                  A. 3D Scene Segmentation
                </h5>
                <img src="assets/imgs/sam_new.jpg" alt="Overview" width="75%">
                <br><br>
                <p class="text-left">
                  An Overview of 3D Raw Data Segmentation. Initially, we generate the depth image and RGB image by rendering a bird's-eye view of the scene. Users have the option to employ SAM for segmenting top-view images through manual selection or fully automated methods. Subsequently, the 3D model is sliced using Blender Python, and CLIP is utilized to filter out non-architectural categories.
                </p>
                <!-- <br> -->

                <h5  class="text-center"> 
                  B. Images Rendering
                </h5>
                <img src="assets/imgs/render.jpg" alt="Overview" width="60%">
                <br><br>
                <p class="text-left">
                  An example of the image rendering part of the toolchain. We offer the random mode (left) and trajectory mode (right) for users to choose from.
                </p>

                <h5  class="text-center"> 
                  C. Descriptive Texts
                </h5>
                <img src="assets/imgs/text.jpg" alt="Overview" width="60%">
                <br><br>
                <p class="text-left">
                  An example of the descriptive text render part of toolchain. we utilize BLIP to perform zero-shot image-to-text generation.
                </p>
                <!-- <br> -->
                  <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                      <source src="video/teaser.mp4" type="video/mp4">
                  </video> -->

              <!-- <p class="text-left">
                &nbsp;&nbsp; We use an advanced AI model segment anything mobile(SAM)  that has shown excellent performance in segmenting complex and diverse images with the ability to perform panoramic segmentation. The front end applies it to segment the bird-eye view image of the 3D scene. The back end uses the Blender Python to segment the 3D scene according to the boundary returned by the front end.
                <br>
                &nbsp;&nbsp; In the automatic segmentation method, since SAM does not have the semantic information of the scene, we will inevitably encounter some non-architectural structures (such as roads, trees, etc.). In consideration of improving the segmentation quality, we use contrastive language-image pre-training model(CLIP) to remove these non-architectural structures. CLIP is a multi-modal pre-training model. It uses more detailed image text descriptions, which can help vision encoder learn better visual features with stronger generalization capabilities. We first use SAMto segment all the scenes and use CLIP to judge whether it belongs to the building through the image rendered by the scene, to filter out the non-architectural structures.
                <br>
                &nbsp;&nbsp; In the manual segmentation method, users have the option to select the desired area either by using a rectangle tool in the bird-eye view or by clicking on the segmented area identified by SAM. This enables them to directly access the corresponding detailed 3D scene.
              </p> -->

            </div>
          </div>
        </div>
    </div>
  </section>
  <!-- <br> -->

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h4>Example of dataset</h4>
                <hr style="margin-top:0px">
                <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/gso_comparison.mp4" type="video/mp4">
                </video> -->
                <!-- <h6 style="color:#8899a5" class="text-left"> 
                  We propose MASSTAR, a multi-modal large-scale scene dataset with a versatile toolchain for surface prediction and completion.
                </h6> -->
                <img src="assets/imgs/rotation_new.gif" alt="ablation-representation" width="80%">
                <!-- <br> -->
                <!-- <p class="text-center">
                   result 
                </p> -->
                <!-- <br> -->
                <!-- <img src="assets/imgs/table.png" alt="ablation-representation" width="100%">
                <p class="text-center">
                 the ouput .
                </p>
                <br> -->
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h4>Surface Completion Benchmark</h4>
                <hr style="margin-top:0px">
                <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/gso_comparison.mp4" type="video/mp4">
                </video> -->
                <h6 style="color:#8899a5" class="text-left"> 
                  We conducted a comparative analysis of three surface prediction and completion algorithms, including SPM, PCN, and XMFnet.
                </h6>
                <img src="assets/imgs/result.gif" alt="ablation-representation" width="90%">
                <!-- <br>
                <p class="text-center">
                   result 
                </p> -->
                <br>
                <!-- <div style="display: flex;">
                  <div style="display: flex; flex-direction: column;flex-direction: column;
                  justify-content: space-between;">
                    <img src="assets/imgs/infer_time.png" alt="ablation-representation" width="90%">
                    <h8 class="text-center">
                      Time Efficiency and Resource Consumption of Each Method 
                    </h8>
                  </div>
                  <div style="display: flex; flex-direction: column;flex-direction: column;
                  justify-content: space-between;">
                    <img src="assets/imgs/eval.png" alt="ablation-representation" width="90%">
                    <h7 class="text-center">
                      The Quality of Prediction Result of Each Method 
                    </h7>
                  </div>
                </div> -->
                <!-- <img src="assets/imgs/table.png" alt="ablation-representation" width="100%">
                <p class="text-center">
                 the ouput .
                </p>
                <br> -->
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  
  <!-- <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h2>Benchmark results</h2>
                <hr style="margin-top:0px">
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/more_gso.mp4" type="video/mp4">
                <img src="assets/imgs/result2.jpg" alt="ablation-representation" width="100%">
                </video>
                <p class="text-center">
                  More results from GSO dataset.
                </p>
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/more_gen.mp4" type="video/mp4">
                </video>
                <p class="text-center">
                  More results from generated images.
                </p>
            </div>
          </div>
        </div>
    </div>
  </section> -->
  <!-- <br> -->

  <!-- <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h2>Ablation</h2>
              <hr style="margin-top:0px">
              <img src="assets/imgs/result2.jpg" alt="ablation-representation" width="100%">
              <p class="text-center">
                Qualitative comparison between different 3D representations: (1) naive generalizable 3D Gaussian (3DG), (2) Triplane-NeRF and (3) Triplane-Guassian.
              </p>
              <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="video/ab.mp4" type="video/mp4">
              </video>
              <img src="assets/imgs/plot_result2.jpg" alt="ablation-method" width="100%">
              <p class="text-center">
                Ablation experiments to assess the effect of Projection-aware Condition (P.C.) and Geometry-aware Encoding (G.E.).
              </p>
            </div>
        </div>
        </div>
    </div>
  </section>
  <br> -->

  <!-- citing -->
  <div class="container">
    <div class="warp-container">
    <div class="row">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{,
    title={MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion},
    author={Guiyong Zheng, Jinqi Jiang, Chen Feng, Shaojie Shen, Boyu Zhou},
    journal={},
    year={2024}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
    This website is adapted from <a href="https://liuyuan-pal.github.io/SyncDreamer/" target="_blank">SyncDreamer</a>.
  </footer>

</body>
</html>
