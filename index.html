
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/custom.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
    <script src="js/google-code-prettify/prettify.js"></script> -->
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion</h2>
            <h5 style="color:#5a6268;">Submitted to IROS 2024</h5>
            <hr>
            <h6 style="line-height: 26px;">
              <a href="http://sysu-star.com/people/" target="_blank">Guiyong Zheng</a><sup>1,4,*</sup>,
              <a href="" target="_blank">Jinqi Jiang</a><sup>1,3,*</sup>,
              <a href="https://chen-albert-feng.github.io/AlbertFeng.github.io/" target="_blank">Chen Feng</a><sup>2,*</sup>,
              <a href="https://uav.hkust.edu.hk/group/" target="_blank">Shaojie Shen</a><sup>2</sup>,
              <a href="http://sysu-star.com/people/" target="_blank">Boyu Zhou</a><sup>1,†</sup>
            </h6>
          <p>
            <h7 style="line-height: 2px;">
              <sup>1</sup> Sun Yat-Sen University. &nbsp;&nbsp;
              <sup>2</sup> The Hong Kong University of Science and Technology. &nbsp;&nbsp;
              <br>
              <sup>3</sup> Harbin Institute of Technology. &nbsp;&nbsp;
              <sup>4</sup> Xidian University. &nbsp;&nbsp;
              <br>
            </h7>
              <sup>*</sup>Equal Contribution &nbsp;&nbsp;
              <sup>†</sup>Corresponding Authors
          </p>

          <div class="row justify-content-center">
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="" role="button"  target="_blank">
                  <i class="fa fa-file"></i> Paper</a> </p>
            </div>
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/SYSU-STAR/MASSTAR" role="button"  target="_blank">
                  <i class="fa fa-github-alt"></i> Code </a> </p>
            </div>
            <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://zenodo.org/records/8416679" role="button"  target="_blank">
                  <i class="fa fa-database"></i> Datasets </a> </p>
            </div>
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.bilibili.com/video/BV1fy4y1F7Z6/?spm_id_from=333.999.0.0&vd_source=7d9ba13550e9ec24b6bf69d5c3ff3b37" role="button"  target="_blank">
                <i class="fa fa-video-camera"></i> Video </a> </p>
          </div>
            <!-- <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/SYSU-STAR/MASSTAR" role="button"  target="_blank">
                  <i class="fa fa-desktop"></i> Demo </a> </p>
            </div> -->
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h4>Abstract</h4>
                <hr style="margin-top:0px">
                <h6 style="color:#8899a5" class="text-left"> 
                  We propose <strong>MASSTAR</strong>, a <strong>m</strong>ulti-modal l<strong>a</strong>rge-scale <strong>s</strong>cene dataset with a ver<strong>s</strong>atile <strong>t</strong>oolchain for surf<strong>a</strong>ce p<strong>r</strong>ediction and completion. 
                </h6>
                  <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/head.jpg" alt="Overview" width="70%">
                  <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                      <source src="video/teaser.mp4" type="video/mp4">
                  </video> -->
                  <br>
                  <br>
              <p class="text-left">
                <strong>
                  Surface prediction and completion have been widely studied in various applications. Recently, research in surface completion has evolved from small objects to complex large-scale scenes. As a result, researchers have begun increasing the volume of data and leveraging a greater variety of data modalities including rendered RGB images, descriptive text, depth images, etc, to enhance algorithm performance. However, existing datasets suffer from a deficiency in the amounts of scene-level models along with the corresponding multi-modal information. Therefore, a method to scale the datasets and generate multi-modal information in them efficiently is essential.
                  To bridge this research gap, we propose MASSTAR: a multi-modal large-scale scene dataset with a versatile toolchain for surface prediction and completion. We develop a versatile and efficient toolchain for processing the raw 3D data from the environments. It screens out a set of fine-grained scene models and generates the corresponding multi-modal data. Utilizing the toolchain, we then generate an example dataset composed of over a thousand scene-level models with partial real-world data added.
                  We compare MASSTAR with the existing datasets, which validates its superiority: the ability to efficiently extract high-quality models from complex scenarios to expand the dataset. Additionally, several representative surface completion algorithms are benchmarked on MASSTAR, which reveals that existing algorithms can hardly deal with scene-level completion.
                </strong>
              </p>
            </div>
          </div>
        </div>
    </div>
  </section>
  <!-- <br> -->



<!-- toolChain example -->
<section>
  <div class="container">
      <div class="warp-container">
        <div class="row">
          <div class="col-12 text-center">
            <h4>Multi-modal Dataset Generation Toolchain</h4>
              <hr style="margin-top:0px">
              <!-- <h6 style="color:#8899a5" class="text-left"> 
                TGS enables fast reconstruction from single-view image. It builds the 3D representation upon a hybrid Triplane-Gaussian representation by evaluating a transformer-based framework, from which 3D Gaussians would be decoded.
              </h6> -->
              
              <h5  class="text-center"> 
                A. 3D Scene Segmentation
              </h5>
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/sam_new.jpg" alt="Overview" width="75%">
              <br><br>
              <p class="text-center">
                An overview of 3D scene segmentation. Initially, we generate the depth image and RGB image by rendering a bird's-eye view of each scene. Users have the option to employ SAM for segmenting top-view images in manual mode or automatic mode. Subsequently, the 3D mesh model is sliced using Blender, and then CLIP is utilized to filter out non-architectural categories.
              </p>
              <!-- <br> -->

              <h5  class="text-center"> 
                B. Images Rendering
              </h5>
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/render.jpg" alt="Overview" width="75%">
              <br><br>
              <p class="text-center">
                An example of the image rendering part of the toolchain. We offer the random mode (left) and trajectory mode (right) for users.
              </p>

              <h5  class="text-center"> 
                C. Descriptive Texts Generation
              </h5>
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/text_new.jpg" alt="Overview" width="75%">
              <br><br>
              <p class="text-center">
                An example of the descriptive text rendering part of the toolchain. BLIP is employed to perform zero-shot image-to-text generation.
              </p>

              <h5  class="text-center"> 
                D. Partial Point Clouds Generation
              </h5>
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/partial_gif.gif" alt="Overview" width="85%">
              <br>
              <p class="text-center">
                An example of the partial point cloud render part of the toolchain.
              </p>
              <!-- <br> -->
                <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/teaser.mp4" type="video/mp4">
                </video> -->

            <!-- <p class="text-left">
              &nbsp;&nbsp; We use an advanced AI model segment anything mobile(SAM)  that has shown excellent performance in segmenting complex and diverse images with the ability to perform panoramic segmentation. The front end applies it to segment the bird-eye view image of the 3D scene. The back end uses the Blender Python to segment the 3D scene according to the boundary returned by the front end.
              <br>
              &nbsp;&nbsp; In the automatic segmentation method, since SAM does not have the semantic information of the scene, we will inevitably encounter some non-architectural structures (such as roads, trees, etc.). In consideration of improving the segmentation quality, we use contrastive language-image pre-training model(CLIP) to remove these non-architectural structures. CLIP is a multi-modal pre-training model. It uses more detailed image text descriptions, which can help vision encoder learn better visual features with stronger generalization capabilities. We first use SAMto segment all the scenes and use CLIP to judge whether it belongs to the building through the image rendered by the scene, to filter out the non-architectural structures.
              <br>
              &nbsp;&nbsp; In the manual segmentation method, users have the option to select the desired area either by using a rectangle tool in the bird-eye view or by clicking on the segmented area identified by SAM. This enables them to directly access the corresponding detailed 3D scene.
            </p> -->

          </div>
        </div>
      </div>
  </div>
</section>
<!-- <br> -->

<!-- Method -->
<section>
  <div class="container">
      <div class="warp-container">
        <div class="row">
          <div class="col-12 text-center">
            <h4>Toolchain Tested on Existing Datasets</h4>
              <hr style="margin-top:0px">
              <!-- <h6 style="color:#8899a5" class="text-left"> 
                TGS enables fast reconstruction from single-view image. It builds the 3D representation upon a hybrid Triplane-Gaussian representation by evaluating a transformer-based framework, from which 3D Gaussians would be decoded.
              </h6> -->
<!--                 
              <h5  class="text-center"> 
                A. 3D Scene Segmentation
              </h5> -->
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/toolchain_example.gif" alt="Overview" width="100%">
              <br><br>
              <p class="text-center">
                Some examples of independent data processing using the proposed toolchain on the existing datasets.
              </p>

          </div>
        </div>
      </div>
  </div>
</section>
<!-- <br> -->

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h4>Example of dataset</h4>
                <hr style="margin-top:0px">
                <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/gso_comparison.mp4" type="video/mp4">
                </video> -->
                <h6 style="color:#8899a5" class="text-center"> 
                  Some high-quality and real-world models.
                </h6>
                <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/model_realworld.gif" alt="ablation-representation" width="100%">
                <br>
                <h6 style="color:#8899a5" class="text-center"> 
                  More Scene-level models.
                </h6>
                <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/model_more.gif" alt="ablation-representation" width="100%">
                <!-- <p class="text-center">
                   result 
                </p> -->
                <!-- <br> -->
                <!-- <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/table.png" alt="ablation-representation" width="100%">
                <p class="text-center">
                 the ouput .
                </p>
                <br> -->
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h4>Surface Completion Benchmark</h4>
                <hr style="margin-top:0px">
                <!-- <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/gso_comparison.mp4" type="video/mp4">
                </video> -->
                <h6 style="color:#8899a5" class="text-left"> 
                  We conducted a comparative analysis of three surface prediction and completion algorithms, including SPM, PCN, and XMFnet.
                </h6>
                <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/result.jpg" alt="ablation-representation" width="99%">
                <!-- <br>
                <p class="text-center">
                   result 
                </p> -->
                <br>
                <!-- <div style="display: flex;">
                  <div style="display: flex; flex-direction: column;flex-direction: column;
                  justify-content: space-between;">
                    <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/infer_time.png" alt="ablation-representation" width="90%">
                    <h8 class="text-center">
                      Time Efficiency and Resource Consumption of Each Method 
                    </h8>
                  </div>
                  <div style="display: flex; flex-direction: column;flex-direction: column;
                  justify-content: space-between;">
                    <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/eval.png" alt="ablation-representation" width="90%">
                    <h7 class="text-center">
                      The Quality of Prediction Result of Each Method 
                    </h7>
                  </div>
                </div> -->
                <!-- <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/table.png" alt="ablation-representation" width="100%">
                <p class="text-center">
                 the ouput .
                </p>
                <br> -->
            </div>
        </div>
        </div>
    </div>
  </section>
  <br>

  
  <!-- <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
                <h2>Benchmark results</h2>
                <hr style="margin-top:0px">
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/more_gso.mp4" type="video/mp4">
                <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/result2.jpg" alt="ablation-representation" width="100%">
                </video>
                <p class="text-center">
                  More results from GSO dataset.
                </p>
                <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                    <source src="video/more_gen.mp4" type="video/mp4">
                </video>
                <p class="text-center">
                  More results from generated images.
                </p>
            </div>
          </div>
        </div>
    </div>
  </section> -->
  <!-- <br> -->

  <!-- <section>
    <div class="container">
        <div class="warp-container">
          <div class="row">
            <div class="col-12 text-center">
              <h2>Ablation</h2>
              <hr style="margin-top:0px">
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/result2.jpg" alt="ablation-representation" width="100%">
              <p class="text-center">
                Qualitative comparison between different 3D representations: (1) naive generalizable 3D Gaussian (3DG), (2) Triplane-NeRF and (3) Triplane-Guassian.
              </p>
              <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="video/ab.mp4" type="video/mp4">
              </video>
              <img src="https://raw.githubusercontent.com/SYSU-STAR/MASSTAR/gh_page/assets/imgs/plot_result2.jpg" alt="ablation-method" width="100%">
              <p class="text-center">
                Ablation experiments to assess the effect of Projection-aware Condition (P.C.) and Geometry-aware Encoding (G.E.).
              </p>
            </div>
        </div>
        </div>
    </div>
  </section>
  <br> -->

  <!-- citing -->
  <div class="container">
    <div class="warp-container">
    <div class="row">
      <div class="col-12">
          <h3>BibTeX</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{,
    title={MASSTAR: A Multi-Modal Large-Scale Scene Dataset with a Versatile Toolchain for Surface Prediction and Completion},
    author={Guiyong Zheng, Jinqi Jiang, Chen Feng, Shaojie Shen, Boyu Zhou},
    journal={},
    year={2024}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>
  
  </div>
  <div class="container">
  <footer class="text-center" style="margin-bottom:10px">
    This website is adapted from <a href="https://liuyuan-pal.github.io/SyncDreamer/" target="_blank">SyncDreamer</a>.
  </footer>
</div>
</body>
</html>
